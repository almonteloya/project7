{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "eeecfbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nn.nn import NeuralNetwork\n",
    "#import pytest \n",
    "#from nn.preprocess import one_hot_encode_seqs, sample_seqs\n",
    "\n",
    "\n",
    "# TODO: Write your test functions and associated docstrings below.\n",
    "\n",
    "\n",
    "## First we write a very simple network\n",
    "nn_ar = [{'input_dim': 1, 'output_dim':2, 'activation' : 'Relu'}, {'input_dim': 2, 'output_dim': 1, 'activation': 'Relu'}]\n",
    "lr = .1\n",
    "seed = 42\n",
    "batch_size = 50\n",
    "epochs = 20\n",
    "loss_function = \"mse\"\n",
    "\n",
    "testpy = NeuralNetwork(nn_ar,lr, seed, batch_size,epochs,loss_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "33497150",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d5cbfbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= np.array([[2],[1]])\n",
    "simpleW = np.array([[.5],[-.5]])\n",
    "testpy._param_dict['W1'] = simpleW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f27c81af",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleb = np.array([[1],[.5]])\n",
    "testpy._param_dict['b1'] = simpleb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "91e315c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The formula for Z = A_prev.dot(W_curr.T) + b_curr.T\n",
    "## Did it in R this is the anwer\n",
    "anwer_R = np.array([[ 2. , -0.5],\n",
    "                   [ 1.5,  0. ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bbd7b75c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_curr, cache = testpy.forward(X)\n",
    "np.all(cache['Z1'] == anwer_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b12d10ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Single forward\n",
    "## We now have the result for Z \n",
    "## Let's try with Relu \n",
    "## Since we know if its less than 0 then 0 if not then the number from Z\n",
    "annswer_A= np.array([[2. , 0. ],\n",
    "                    [1.5, 0. ]])\n",
    "A_, Z_ = testpy._single_forward(simpleW, simpleb, X, \"Relu\")\n",
    "np.all(A_ == annswer_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b249e9a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predict \n",
    " ## since we already proved forward is correct, we need to prove they give the same results \n",
    "y_pred, _ = testpy.predict(X)\n",
    "y_predf,_ = testpy.forward(X)\n",
    "np.all(y_pred == y_predf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b1dbe97e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## _mean_squared_error_backprop\n",
    "## The formula for the derivate is -(2*(y - y_pred)/len(y))\n",
    "y = np.array([[0.5],\n",
    "       [-0.5]])\n",
    "## I used R to caluculate it\n",
    "mse_R = np.array([[-0.3788796],[0.6219598]])\n",
    "mse_py = testpy._mean_squared_error_backprop(y,y_pred)\n",
    "## Since we have a difference of decimals we'll round them\n",
    "mse_R_r = np.around(mse_R, decimals=7) \n",
    "mse_P_r = np.around(mse_py, decimals=7) \n",
    "np.all(mse_R_r - mse_P_r == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0e7246ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2. , -0.5],\n",
       "       [ 1.5,  0. ]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Now back prop \n",
    "\n",
    "testpy._single_backprop(\n",
    "W_curr= simpleW,\n",
    "b_curr= simpleb,\n",
    "Z_curr = cache['Z1'],\n",
    "A_prev=  cache['A1'],\n",
    "dA_curr= testpy._mean_squared_error_backprop(y,y_pred),\n",
    "activation_curr= 'Relu')\n",
    "\n",
    "## The formula for Partial derivative of loss function / weight matrix = \n",
    "#  dL_dA_dL_dZ.T.dot(A_prev)\n",
    "\n",
    "## dL_dA_dL_dZ is obtained by dA * backprop activation of Z\n",
    "cache['Z1'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3a12e78e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2. , 0. ],\n",
       "       [1.5, 1. ]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = np.where(cache['Z1'] == 0, 1, cache['Z1'] )\n",
    "\n",
    "Z= np.where(cache['Z1'] == 0, 1, cache['Z1'] )\n",
    "np.where(Z > 0, Z, 0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
